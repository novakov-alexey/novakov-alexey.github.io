+++
title="Linear Regression with Adam Optimizer"
date=2021-02-20
draft = false

[extra]
category="blog"
toc = true

[taxonomies]
tags = ["deep learning", "machine learning", "linear regression", "Adam", "Picta"]
categories = ["scala"]
+++

[Adam](https://arxiv.org/pdf/1412.6980.pdf) is one more optimization algorithm used in neural netwroks. It is based on adaptive estimates of lower-order moments. It has more hyperparameters than classic Gradient Descent to tune externally:

 Good default settings for the tested machine learning problems are:
  - Œ± =  0.001, // learning rate. We have already seen this one in classic Gradient Descent.
  - Œ≤1 = 0.9,
  - Œ≤2 = 0.999
  - eps= 10‚àí8.  
  
Values on the right-hand side are proposed in the paper. However, you can tune them on your data, change batch size and other parameters which
depend the Adam parameters.

# Algorithm

Let's look at original algorithm and then try to implement it in code:


All operations on vectors are element-wise.  With Œ≤<sub>1</sub><sup>t</sup> and Œ≤<sub>2</sub><sup>t</sup> 
we denote Œ≤1 and Œ≤2 to the power of `t`.

__Require__: Œ±: Stepsize

__Require:__ Œ≤<sub>1</sub>, Œ≤<sub>2</sub> ‚àà[0,1): Exponential decay rates for the moment estimates

__Require__: f(Œ∏): Stochastic objective function with parameters Œ∏ 

__Require__: Œ∏<sub>0</sub>: Initial parameter vector

  m<sub>0</sub> ‚Üê 0 (Initialize 1st moment vector)

  v<sub>0</sub> ‚Üê 0 (Initialize 2nd moment vector)

  t ‚Üê 0 (Initialize timestep)

  __while__ Œ∏<sub>t</sub> not converged do 

  t ‚Üê t + 1

  gt ‚Üê ‚àáŒ∏f<sub>t</sub> (Œ∏<sub>t‚àí1</sub>) (Get gradients w.r.t. stochastic objective at timestep t)

  mt ‚Üê Œ≤<sub>1</sub> ¬∑ m<sub>t‚àí1</sub> + (1‚àíŒ≤<sub>1</sub>) ¬∑ gt (Update biased first moment estimate)

  vt ‚Üê Œ≤<sub>2</sub> ¬∑ v<sub>t</sub>‚àí1 + (1‚àíŒ≤<sub>2</sub>) ¬∑ g<sup>2</sup>t (Update biased second raw moment estimate)ÃÇ 

  mt ‚Üê m<sub>t</sub> / (1‚àíŒ≤t<sub>1</sub>) (Compute bias-corrected first moment estimate)ÃÇ

  v<sub>t</sub> ‚Üê v<sub>t</sub> / (1‚àíŒ≤t<sub>2</sub>) (Compute bias-corrected second raw moment estimate)

  Œ∏<sub>t</sub> ‚Üê Œ∏<sub>t‚àí1</sub>‚àíŒ±¬∑ÃÇmt/(‚àöÃÇv<sub>t</sub> +eps) (Update parameters)

  __end while__

  __return__ Œ∏<sub>t</sub> (Resulting parameters)

At the last line we update paramateres based on long chain of formulas which incorporate gradient and moments.


# Implementation

## Changes to existing library

In order to implement `Adam` for multi-layer neural network with backpropogation we will need to translate above algorithm to
linear algebra and Tensor API that we wrote in [previous articles](../ann-in-scala-2). Apart from that we would need to 
add additional state to `Layer` type:

```scala
case class Layer[T](
    w: Tensor[T],
    b: Tensor[T],
    f: ActivationFunc[T] = ActivationFuncApi.noActivation[T],
    units: Int = 1,
    state: Option[OptimizerState[T]] = None // additional property for an optimizer
)
```

We assume that any optimizer may bring its own state besied the usual weight and bias matrices, so we model 
new property as trait and add implementation for Adam:

```scala
sealed trait OptimizerState[T]

case class AdamState[T](
    mw: Tensor[T], // 1st moment equal in shape to weight
    vw: Tensor[T], // 2nd moment equal in shape to weight
    mb: Tensor[T], // 1st moment equal in shape to bias
    vb: Tensor[T]  // 2nd moment equal in shape to bias
) extends OptimizerState[T]
```

We aslo need to initialize this state properties with zeros. For that, we extand layer construction code to let the optimizer type class
to init its properties on its own.

First we change `add` method the `Sequential` model type:

```scala
def add(layer: LayerCfg[T]): Sequential[T, U] =
  copy(layerStack = (inputs) => {
    val currentLayers = layerStack(inputs)
    val prevInput = currentLayers.lastOption.map(_.units).getOrElse(inputs)
    val w = random2D(prevInput, layer.units)
    val b = zeros(layer.units)
    // new line to init state for the choosen optimizer
    val optimizerState = optimizer.initState(w, b) 
    (currentLayers :+ Layer(w, b, layer.f, layer.units, optimizerState))
  })
```
Optimizer trait now gets additional method:

```scala
  def initState[T: ClassTag: Numeric](
      w: Tensor[T], b: Tensor[T]
  ): Option[OptimizerState[T]] = None
```

Adam implementation for `initState`:

```scala
override def initState[T: ClassTag: Numeric](
    w: Tensor[T], 
    b: Tensor[T]): Option[OptimizerState[T]] =
  Some(AdamState[T](w.zero, w.zero, b.zero, b.zero))
```

`Tensor.zero` method creates new tensor with zeros using the same shape as original tensor.

Also, we need to keep Adam hyperparameters somewhere. Let's crete `OptimizerCfg` class and Adam extension in it.
We could also go into `trait` way, but I decided to make it dirty first time:

```scala
case class OptimizerCfg[T: ClassTag: Fractional](
  learningRate: T,
  clip: GradientClipping[T] = GradientClippingApi.noClipping[T],
  adam: AdamCfg[T]
)

case class AdamCfg[T: ClassTag](b1: T, b2: T, eps: T)
```

## Update Weights using Adam

We now have all abstraction in place as well as all parameters to implement Adam optimizer.
In fact, first part to calculate gradient (partial derrivative) will be the same as in classic gradient descent algorithm.
Second part will be `Adam's` own stuff:

```scala
override def updateWeights[T: ClassTag](
  layers: List[Layer[T]],
  activations: List[Activation[T]],
  error: Tensor[T],
  c: OptimizerCfg[T],
  timestep: Int
)(using n: Fractional[T]): List[Layer[T]] =
  val AdamCfg(b1, b2, eps) = c.adam        

  def correction(gradient: Tensor[T], m: Tensor[T], v: Tensor[T]) =        
    val mt = (b1 * m) + ((n.one - b1) * gradient)
    val vt = (b2 * v) + ((n.one - b2) * gradient.sqr)        
    val mHat = mt :/ (n.one - (b1 ** timestep))
    val vHat = vt :/ (n.one - (b2 ** timestep))            

    val corr = c.learningRate *: (mHat / (vHat.sqrt + eps))
    (corr, mt, vt)
  
  layers
    .zip(activations)
    .foldRight(
      List.empty[Layer[T]],
      error,
      None: Option[Tensor[T]]          
    ) {             
      case (
          (Layer(w, b, f, u, Some(AdamState(mw, vw, mb, vb))), Activation(x, z, _)),
          (ls, prevDelta, prevWeight)
        ) =>            
        val delta = (prevWeight match 
          case Some(pw) => prevDelta * pw.T
          case None     => prevDelta
        ) multiply f.derivative(z)        
        val wGradient = c.clip(x.T * delta)
        val bGradient = c.clip(delta).sum
        
        // Adam                        
        val (corrW, weightM, weightV) = correction(wGradient, mw, vw)
        val newWeight = w - corrW

        val (corrB, biasM, biasV) = correction(bGradient.asT, mb, vb)
        val newBias = b - corrB

        val adamState = Some(AdamState(weightM, weightV, biasM, biasV))
        val updated = Layer(newWeight, newBias, f, u, adamState) +: ls              
        (updated, delta, Some(w))
        
      case s => sys.error(s"Adam optimizer require state, but was:\n$s")
    }
    ._1    
```

The difference with classis gradient optimizer is:

1. `timestep` is an index acorss all training epochs and batched: `[1 .. epochs * data.length / batchsize]`
1. `correction` function that goes after Adam paper to calculate final learning rate based on the weight or bias gradient.
1. We keep Adam moments for weight and bias `AdamState` as part of the Layer state across all learning epochs.

There is an extension to Tensor API I have added to support elementwise operations like:

1. division `def :/(that: T): Tensor[T]`
1. multiplication `(t: T) def *:(that: Tensor[T]): Tensor[T]`
1. power: `def :**(to: Int): Tensor[T]`
1. square: `def sqr: Tensor[T] = TensorOps.pow(t, 2)`
1. sqrt: `def sqrt: Tensor[T] = TensorOps.sqrt(t)`

# Visualization

We are going to visualize Adam gradient trace to global minimum using Picta. So all we do is constructing ANN with Adam type parameter:

```scala
val ann = Sequential[Double, Adam](
    meanSquareError,
    learningRate = 0.0012f,    
    batchSize = 16,
    gradientClipping = clipByValue(5.0d)
  ).add(Dense())    
```    

Loss surface:

{{ resize_image(path="adam-optimizer/loss-surface.png", width=800, height=600, op="fit") }}

Also, we going to compare it on the same data with classic Gradient Descent:

{{ resize_image(path="adam-optimizer/loss-contour.png", width=800, height=600, op="fit") }}

Adam gradient starts a bit differently then classic gradient descent. Eventually, they both converges in the same point.

If we compare the speed of finding global minimum, then on my data and on the same learning hyper-parameters, classic Gradient Descent is faster:

{{ resize_image(path="adam-optimizer/gradient-trace-2.png", width=800, height=600, op="fit") }}
{{ resize_image(path="adam-optimizer/gradient-trace-3.png", width=800, height=600, op="fit") }}
{{ resize_image(path="adam-optimizer/gradient-trace.png", width=800, height=600, op="fit") }}
{{ resize_image(path="adam-optimizer/gradient-trace-4.png", width=800, height=600, op="fit") }}

We can see that orange line is sligthly behind the blue one. Around `9th` learning epoch they are both in the same position.

# Summary

We could easily extend existing library with one more optimizer such as Adam. It is quite popular optimizer nowadays as it shows
good result in the paper. Anyway, it did not show better results on my data comparing to classic gradient descent algorithm.
My experiment is not proving that Adam is not good, but it is just showing that in real life you need to experiment with 
different weight optimizers. Also, you should tune hyperparameters for each algorithm separately, i.e. reuse of the same hyperparameters
might not help to get the best results out of another optimizer you are currently trying.


# Links

1. [Source code - Optimizers](https://github.com/novakov-alexey/deep-learning-scala/blob/master/src/main/scala/ml/network/optimizers.scala)
1. [Reference Implementation for perceptron](https://machinelearningmastery.com/adam-optimization-from-scratch/)
1. [Paper: Adam: a method for stochastic optimization](https://arxiv.org/pdf/1412.6980.pdf)